###### 大模型微调实战

### 微调原理

> 想象一下，你有一个超大的玩具，现在你想改造这个超大的玩具。但是，**对整个玩具进行全面的改动会非常昂贵**。

※ 因此，你找到了一种叫 **LoRA** 的方法：**只对玩具中的某些零件进行改动，而不是对整个玩具进行全面改动**。

※ 而 **QLoRA** 是 LoRA 的一种改进：如果你手里只有一把生锈的螺丝刀，也能改造你的玩具。

### 平台

Ubuntu + Anaconda + CUDA/CUDNN + 8GB nvidia显卡

### 安装和微调步骤：

参考：

[tutorial/xtuner at main · InternLM/tutorial (github.com)](https://github.com/InternLM/tutorial/tree/main/xtuner)

[tutorial/xtuner/self.md at main · InternLM/tutorial (github.com)](https://github.com/InternLM/tutorial/blob/main/xtuner/self.md)